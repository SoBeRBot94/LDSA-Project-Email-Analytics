# vi: set ft=yaml.ansible :
---
- hosts: Master
  become: yes
  remote_user: ubuntu
  gather_facts: no
  pre_tasks:
    - name: "Update Aptitude Cache"
      raw: sudo apt-get -y update

    - name: "Task 0: Install Python2"
      raw: sudo apt-get -y install python

  vars:
    new_hostname: spark-master

  tasks:

  - name: "Task 1: Set Hostname [1] ---> Hostnamectl"
    hostname:
      name: "{{ new_hostname }}"

  - name: "Task 2: Set Hostname [2] ---> /etc/hosts"
    template:
      src: ./hosts.j2
      dest: /etc/hosts
      mode: 0644
      owner: root
      group: root
      force: yes

  - name: "Task 3: Set Hostname [3] ---> Replace /etc/hostname"
    replace:
      path: /etc/hostname
      regexp: '[a-z]*-*[a-z]*'
      replace: "{{ new_hostname }}"

  - name: "Task 4: Set Hostname [4] ---> Set Persistance"
    replace:
      path: /etc/cloud/cloud.cfg
      regexp: 'preserve_hostname: false'
      replace: 'preserve_hostname: yes'

  - name: "Task 5: Create SSH RSA Keys"
    command: ssh-keygen -t rsa -f /home/ubuntu/.ssh/id_rsa -P ""
    args:
      creates: /home/ubuntu/.ssh/id_rsa

  - name: "Task 6: Chown SSH RSA Keys"
    file:
      path: "{{ item }}"
      state: touch
      owner: ubuntu
      group: ubuntu
      mode: 0600
    with_items:
      - /home/ubuntu/.ssh/id_rsa
      - /home/ubuntu/.ssh/id_rsa.pub

  - name: "Task 7: Fetch SSH RSA KeyFile"
    fetch:
      src: /home/ubuntu/.ssh/id_rsa.pub
      dest: ./id_rsa.pub
      flat: yes

  - name: "Task 8: Deploy SSH RSA Keys"
    become: no
    shell: cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys
    args:
      executable: /bin/bash

  - name: "Task 9: Install Pre-Requisites"
    apt:
      update_cache: yes
      name: "{{ item }}"
      state: present
    with_items:
      - default-jre
      - scala
      - python3
      - python-pip
      - python3-pip
      - supervisor

  - name: "Task 10: Update pip2 & pip3"
    pip:
      name: pip
      extra_args: --upgrade
      executable: "{{ item }}"
    with_items:
      - pip2
      - pip3

  - name: "Task 11: Install Jupyter"
    pip:
      name: jupyter
      state: latest
      executable: pip3

  - name: "Task 12: Create Supervisord log Directory"
    file:
      path: /var/log/supervisor
      state: directory

  - name: "Task 13: Copy Jupyter Supervisor Service Configuration File"
    become: yes
    copy:
      src: ./jupyter.conf
      dest: /etc/supervisor/conf.d/jupyter.conf
      mode: 0644
      owner: ubuntu
      group: ubuntu

  - name: "Task 14: Start Supervisord Service"
    service:
      name: supervisor
      state: restarted

# --------------------------------------------------

- hosts: Slaves
  become: yes
  remote_user: ubuntu
  gather_facts: no
  pre_tasks:
    - name: "Update Aptitude Cache"
      raw: sudo apt-get -y update

    - name: "Task 0: Install Python2"
      raw: sudo apt-get -y install python

  tasks:

  - name: "Task 1: Set Hostname [1] ---> Hostnamectl"
    hostname:
      name: spark-slave-{{ hostname_suffix }}

  - name: "Task 2: Set Hostname [2] ---> /etc/hosts"
    template:
      src: ./slaves_hosts.j2
      dest: /etc/hosts
      mode: 0644
      owner: root
      group: root
      force: yes

  - name: "Task 3: Set Hostname [3] ---> Replace /etc/hostname"
    replace:
      path: /etc/hostname
      regexp: '[a-z]*-*[a-z]*'
      replace: spark-slave-{{ hostname_suffix }}

  - name: "Task 4: Set Hostname [4] ---> Set Persistance"
    replace:
      path: /etc/cloud/cloud.cfg
      regexp: 'preserve_hostname: false'
      replace: 'preserve_hostname: yes'

  - name: "Task 5: Copy RSA KeyFile"
    copy:
      src: ./id_rsa.pub
      dest: /home/ubuntu/.ssh/id_rsa.pub

  - name: "Task 6: Chown RSA Keys"
    file:
      path: "{{ item }}"
      state: touch
      owner: ubuntu
      group: ubuntu
      mode: 0600
    with_items:
      - /home/ubuntu/.ssh/id_rsa.pub

  - name: "Task 8: Deploy SSH RSA Keys"
    become: no
    shell: cat /home/ubuntu/.ssh/id_rsa.pub >> /home/ubuntu/.ssh/authorized_keys
    args:
      executable: /bin/bash
    
  - name: "Task 9: Install Pre-Requisites"
    apt:
      update_cache: yes
      name: "{{ item }}"
      state: present
    with_items:
      - default-jre
      - scala
      - python3
      - python-pip
      - python3-pip

  - name: "Task 10: Update pip2 & pip3"
    pip:
      name: pip
      extra_args: --upgrade
      executable: "{{ item }}"
    with_items:
      - pip2
      - pip3

# --------------------------------------------------

- hosts: Master:Slaves
  become: yes
  remote_user: ubuntu
  gather_facts: yes

  tasks:

  - name: "Task 1: Install PySpark"
    pip:
      name: "{{ item }}"
      state: present
      executable: pip3
    with_items:
      - pyspark==2.3.3
      - ipython

  - name: "Task 2: Setup PySpark Environment"
    become: yes
    lineinfile:
      path: /etc/profile
      line: "{{ item }}"
      state: present
    with_items:
      - export PYSPARK_PYTHON=/usr/bin/python3
      - export PYSPARK_DRIVER_PYTHON=/usr/local/bin/ipython

  - name: "Task 3: Setup JAVA Environment"
    become: yes
    lineinfile:
      path: /etc/profile
      line: "{{ item }}"
      state: present
    with_items:
      - export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/
      - export PATH=$PATH:$JAVA_HOME/bin

  - name: "Task 4: Fetch Spark 2.3.3 - Hadoop 2.7 Tarball"
    get_url:
      url: http://apache.mirrors.spacedump.net/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz
      dest: /home/ubuntu/spark-2.3.3-bin-hadoop2.7.tgz

  - name: "Task 5: Extract Spark TarBall"
    unarchive:
      src: /home/ubuntu/spark-2.3.3-bin-hadoop2.7.tgz
      dest: /home/ubuntu/
      remote_src: yes

  - name: "Task 6: Setup Spark Environment"
    command: cp -r /home/ubuntu/spark-2.3.3-bin-hadoop2.7 /usr/local/spark

  - name: "Task 7: Chown Spark"
    file:
      path: /usr/local/spark
      state: directory
      owner: ubuntu
      group: ubuntu
      recurse: yes

  - name: "Task 8: Setup Spark Environment Variables"
    become: yes
    lineinfile:
      path: /etc/profile
      line: "{{ item }}"
      state: present
    with_items:
      - export SPARK_HOME=/usr/local/spark
      - export PATH=$PATH:$SPARK_HOME/bin

  - name: "Task 9: Configure Spark Cluster Nodes [1] ---> Copy Template File to a new Env File"
    copy:
      src: /usr/local/spark/conf/spark-env.sh.template
      dest: /usr/local/spark/conf/spark-env.sh
      owner: ubuntu
      group: ubuntu
      remote_src: yes

  - name: "Task 10: Configure Spark Cluster Nodes [2] ---> Copy Template File to a new Conf File"
    copy:
      src: /usr/local/spark/conf/spark-defaults.conf.template
      dest: /usr/local/spark/conf/spark-defaults.conf
      owner: ubuntu
      group: ubuntu
      remote_src: yes

  - name: "Task 11: Configure Spark Cluster Nodes [3] ---> Set Spark Cluster Node Configuration Environment Variables"
    lineinfile:
      path: /usr/local/spark/conf/spark-env.sh
      line: "{{ item }}"
      state: present
    with_items:
      - export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/
      - export SPARK_WORKER_CORES=4
      - export SPARK_DRIVER_MEMORY=4G
      - export SPARK_EXECUTOR_MEMORY=8G
      - export SPARK_WORKER_PORT=7078
      - export SPARK_MASTER_PORT=7077

  - name: "Task 12: Configure Spark Cluster Nodes [4] ---> Set Spark Cluster Node Configuration Default Variables"
    lineinfile:
      path: /usr/local/spark/conf/spark-defaults.conf
      line: "{{ item }}"
      state: present
    with_items:
      - spark.ui.port 4040
      - spark.driver.port 9929

  - name: "Task 13: Configure Spark Cluster Nodes [5] ---> Set-up Spark Slaves File"
    lineinfile:
      path: /usr/local/spark/conf/slaves
      create: yes
      line: "{{ item }}"
      state: present
      owner: ubuntu
      group: ubuntu
    with_inventory_hostnames: Slaves

  - set_fact:
      spark_master_public_ip: "{{ item }}"
    with_inventory_hostnames: Master

  - set_fact:
      spark_slave_1_public_ip: "{{ groups['Slaves']|first }}"


  - set_fact:
      spark_slave_2_public_ip: "{{ groups['Slaves']|last }}"

  - name: "Task 14: Setup /etc/hosts"
    become: yes
    lineinfile:
      path: /etc/hosts
      line: "{{ item }}"
      state: present
    with_items:
      - '{{ spark_master_public_ip }} spark-master'
      - '{{ spark_slave_1_public_ip }} spark-slave-1'
      - '{{ spark_slave_2_public_ip }} spark-slave-2'

  - name: "Task 15: Gather Spark Master Host Fact"
    shell: ifconfig | grep inet\ addr | awk 'FNR ==1 {print $2}' | cut -d ':' -f 2
    delegate_to: "{{ item }}"
    with_inventory_hostnames: Master
    register: ipv4

  - set_fact:
      spark_master_host: "{{ item.stdout }}"
    loop: "{{ ipv4.results }}"

  - name: "Task 16: Set Spark Master Host Environment Variable"
    lineinfile:
      path: /usr/local/spark/conf/spark-env.sh
      line: export SPARK_MASTER_HOST={{ spark_master_host }}
      state: present

# --------------------------------------------------

- hosts: Master
  become: yes
  remote_user: ubuntu
  gather_facts: yes

  tasks:

  - name: "Check for Jupyter.log"
    stat:
      path: /home/ubuntu/jupyter.log
    register: log_file

  - name: "Fetch Jupyter Token"
    shell: cat /home/ubuntu/jupyter.log | grep token | cut -d '?' -f 2 | cut -d '=' -f 2 | awk 'FNR == 2 {print $1}'
    register: Jupyter_Token
    when: log_file.stat.exists == True

  - name: "Jupyter Token"
    debug:
      var: Jupyter_Token.stdout_lines
